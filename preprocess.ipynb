{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import parsivar\n",
    "import hazm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    def __init__(self):\n",
    "        self.normalizer = parsivar.Normalizer(pinglish_conversion_needed=True)\n",
    "        self.tokenizer = parsivar.Tokenizer()\n",
    "        self.stemmer = parsivar.FindStems()\n",
    "        self.stop_words = hazm.stopwords_list()\n",
    "        self.to_remove = ['۰', '۱', '۲', '۳', '۴', '۵', '۶', '۷', '۸', '۹',\n",
    "                          '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "                          '(', ')', '[', ']', '«', '»', '<<', '>>', '{', '}',\n",
    "                          '?', '،', '.', ':', '-', '_', '/', '=', '؛', '&', \"%\", \"#\", \"*\",\n",
    "                          'https://', 'http://', 'انتهای پیام', '://']\n",
    "\n",
    "    def normalize(self, sentence):\n",
    "        return self.normalizer.normalize(sentence)\n",
    "\n",
    "    def tokenize(self, sentence):\n",
    "        sentence = sentence.replace(\"انتهای پیام\", \"\")\n",
    "        tokens = self.tokenizer.tokenize_words(sentence)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def stem(self, tokens):\n",
    "        stem_tokens_indexed = []\n",
    "        for token in tokens:\n",
    "            stem_tokens = self.stemmer.convert_to_stem(token)\n",
    "\n",
    "            stem_tokens_indexed.append(stem_tokens)\n",
    "\n",
    "        return stem_tokens_indexed\n",
    "\n",
    "    def redact_stops(self, tokens):\n",
    "        temp = set(self.stop_words)\n",
    "        to_redact1 = [_ for _ in tokens if _ in temp]\n",
    "\n",
    "        temp2 = set(self.to_remove)\n",
    "        to_redact2 = [_ for _ in tokens if _ in temp2]\n",
    "\n",
    "        to_redact = to_redact1 + to_redact2\n",
    "\n",
    "        tokens = list(filter(lambda k: k not in to_redact, tokens))\n",
    "\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def query_preprocess(self, query):\n",
    "\n",
    "        query_normalized = self.normalize(query)\n",
    "        query_tokenized = self.tokenize(query_normalized)\n",
    "        query_stemmed = self.stem(query_tokenized)\n",
    "        return self.redact_stops(query_stemmed).split(\" \")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_parser(query):\n",
    "    preprocess = Preprocess()\n",
    "\n",
    "    phrases = re.findall('\"([^\"]*)\"', query)\n",
    "\n",
    "    nots = []\n",
    "    seen = False\n",
    "    for term in query.split(\" \"):\n",
    "        if seen:\n",
    "            nots.append(term)\n",
    "            seen = False\n",
    "        if term == '!':\n",
    "            seen = True\n",
    "\n",
    "    normal = []\n",
    "    regex = r'\".*?\"|(\\w+)'\n",
    "    matches = re.finditer(regex, query, re.MULTILINE)\n",
    "    for match in matches:\n",
    "        if match.group(1):\n",
    "            if match.group(1) not in nots:\n",
    "                normal.append(match.group(1))\n",
    "\n",
    "    phrases = list(map(preprocess.query_preprocess, phrases))\n",
    "    # nots = list(map(preprocess.query_preprocess, nots))\n",
    "    normal = preprocess.query_preprocess(\" \".join(normal))\n",
    "\n",
    "    return phrases, nots, normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = Preprocess()\n",
    "\n",
    "df = pd.read_json('./data/IR_data_news_12k.json', orient='index')\n",
    "df_processed = df\n",
    "\n",
    "df_processed['content'] = df['content'].apply(preprocess.normalize)\n",
    "\n",
    "df_processed['content'] = df_processed['content'].apply(preprocess.tokenize)\n",
    "\n",
    "df_processed['content'] = df_processed['content'].apply(preprocess.stem)\n",
    "\n",
    "df_processed['content'] = df_processed['content'].apply(preprocess.redact_stops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.to_json('./data/preprocessed.json', orient='index')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ec66bbe782f389edb8615b95e71913b48001538f92dc4d53a6c71a0eff16772"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
